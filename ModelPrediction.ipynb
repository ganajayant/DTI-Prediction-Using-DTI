{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric import data as DATA\n",
    "from torch_geometric.nn import GCNConv, global_max_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a new dataset, which is a list of GCN_DATA convert into dataloader\n",
    "dataset = torch.load('preprocessed_dataset.pt')\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    dataset, test_size=0.2, random_state=42)\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    train_dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DATA.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader = DATA.DataLoader(val_dataset,  shuffle=True, drop_last=True)\n",
    "test_loader = DATA.DataLoader(test_dataset,  shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    class GCNNet(torch.nn.Module):\n",
    "        def __init__(self, n_output=2, n_filters=32, embed_dim=128, num_features_xd=78, num_features_xt=25, output_dim=128, dropout=0.2):\n",
    "\n",
    "            super(GCNNet, self).__init__()\n",
    "\n",
    "            self.n_output = n_output\n",
    "            self.conv1 = GCNConv(num_features_xd, num_features_xd)\n",
    "            self.conv2 = GCNConv(num_features_xd, num_features_xd*2)\n",
    "            self.conv3 = GCNConv(num_features_xd*2, num_features_xd * 4)\n",
    "            self.fc_g1 = torch.nn.Linear(num_features_xd*4, 1024)\n",
    "            self.fc_g2 = torch.nn.Linear(1024, output_dim)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "            self.embedding_xt = nn.Embedding(num_features_xt + 1, embed_dim)\n",
    "            self.conv_xt_1 = nn.Conv1d(\n",
    "                in_channels=1000, out_channels=n_filters, kernel_size=8)\n",
    "            self.fc1_xt = nn.Linear(32*121, output_dim)\n",
    "\n",
    "            self.fc1 = nn.Linear(2*output_dim, 1024)\n",
    "            self.fc2 = nn.Linear(1024, 512)\n",
    "            self.out = nn.Linear(512, self.n_output)\n",
    "\n",
    "        def forward(self, data):\n",
    "            # get graph input\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            # get protein input\n",
    "            target = data.target\n",
    "\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = self.relu(x)\n",
    "\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = self.relu(x)\n",
    "\n",
    "            x = self.conv3(x, edge_index)\n",
    "            x = self.relu(x)\n",
    "            x = global_max_pool(x, batch)       # global max pooling\n",
    "\n",
    "            # flatten\n",
    "            x = self.relu(self.fc_g1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc_g2(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            # 1d conv layers\n",
    "            embedded_xt = self.embedding_xt(target)\n",
    "            conv_xt = self.conv_xt_1(embedded_xt)\n",
    "            # flatten\n",
    "            xt = conv_xt.view(-1, 32 * 121)\n",
    "            xt = self.fc1_xt(xt)\n",
    "\n",
    "            # concat\n",
    "            xc = torch.cat((x, xt), 1)\n",
    "            # add some dense layers\n",
    "            xc = self.fc1(xc)\n",
    "            xc = self.relu(xc)\n",
    "            xc = self.dropout(xc)\n",
    "            xc = self.fc2(xc)\n",
    "            xc = self.relu(xc)\n",
    "            xc = self.dropout(xc)\n",
    "            out = self.out(xc)\n",
    "            return out\n",
    "    model = GCNNet()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    return model, optimizer, loss_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainTheModel(num_epochs=20):\n",
    "    model, optimizer, loss_function = Model()\n",
    "    train_loss = torch.zeros(num_epochs)\n",
    "    valid_loss = torch.zeros(num_epochs)\n",
    "    train_acc = torch.zeros(num_epochs)\n",
    "    valid_acc = torch.zeros(num_epochs)\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = loss_function(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss[epoch] += loss.item()\n",
    "            pred = out.max(1)[1]\n",
    "            train_acc[epoch] += pred.eq(data.y).sum().item()\n",
    "        train_loss[epoch] /= len(train_loader.dataset)\n",
    "        train_acc[epoch] /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        for data in val_loader:\n",
    "            out = model(data)\n",
    "            loss = loss_function(out, data.y)\n",
    "            valid_loss[epoch] += loss.item()\n",
    "            pred = out.max(1)[1]\n",
    "            valid_acc[epoch] += pred.eq(data.y).sum().item()\n",
    "        valid_loss[epoch] /= len(val_loader.dataset)\n",
    "        valid_acc[epoch] /= len(val_loader.dataset)\n",
    "        if (valid_acc[epoch] > best_acc):\n",
    "            best_model = model\n",
    "        print('Epoch: {:03d}, Train Loss: {:.5f}, Train Acc: {:.5f}, Val Loss: {:.5f}, Val Acc: {:.5f}'.format(\n",
    "            epoch, train_loss[epoch], train_acc[epoch], valid_loss[epoch], valid_acc[epoch]))\n",
    "    return best_model, train_loss, valid_loss, train_acc, valid_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_loss, valid_loss, train_acc, valid_acc = TrainTheModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(train_loss.numpy(), color='b', label=\"Training loss\")\n",
    "ax[0].plot(valid_loss.numpy(), color='r', label=\"validation loss\", axes=ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(train_acc.numpy(), color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(valid_acc.numpy(), color='r', label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights of the model to a pickle file\n",
    "torch.save(model.state_dict(), 'model_weights.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "model.load_state_dict(torch.load('model_weights.pt'))\n",
    "model.eval()\n",
    "test_acc = 0\n",
    "precision = 0\n",
    "specificty = 0\n",
    "for data in test_loader:\n",
    "    out = model(data)\n",
    "    pred = out.max(1)[1]\n",
    "    test_acc += pred.eq(data.y).sum().item()\n",
    "    precision += precision_score(data.y, pred)\n",
    "    specificty += recall_score(data.y, pred)\n",
    "precision /= len(test_loader.dataset)\n",
    "specificty /= len(test_loader.dataset)\n",
    "test_acc /= len(test_loader.dataset)\n",
    "print('Precision: {:.5f}'.format(precision))\n",
    "print('Specificty: {:.5f}'.format(specificty))\n",
    "print('Test Accuracy: {:.5f}'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print f1 score\n",
    "f1_score = 2 * (precision * specificty) / (precision + specificty)\n",
    "print('F1 Score: {:.5f}'.format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot AUC-ROC curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for data in test_loader:\n",
    "    out = model(data)\n",
    "    pred = out.max(1)[1]\n",
    "    y_true.append(data.y)\n",
    "    y_pred.append(pred)\n",
    "y_true = torch.cat(y_true)\n",
    "y_pred = torch.cat(y_pred)\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot AUC-PR curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for data in test_loader:\n",
    "    out = model(data)\n",
    "    pred = out.max(1)[1]\n",
    "    y_true.append(data.y)\n",
    "    y_pred.append(pred)\n",
    "y_true = torch.cat(y_true)\n",
    "y_pred = torch.cat(y_pred)\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "average_precision = average_precision_score(y_true, y_pred)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(recall, precision, color='darkorange',\n",
    "         lw=lw, label='PR curve (area = %0.2f)' % average_precision)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b19b46d66d21ed878d7479c740a53ebae5b614c812aabd7ad3de252e90c6a0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
